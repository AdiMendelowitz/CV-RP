{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "name": "ViT_CIFAR10_Training.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Vision Transformer (ViT-Tiny) ‚Äî CIFAR-10\n",
    "**Vision Transformers**\n",
    "\n",
    "| Setting | Value |\n",
    "|---|---|\n",
    "| Model | ViT-Tiny (5.3M params) |\n",
    "| Dataset | CIFAR-10 (50k train / 10k test) |\n",
    "| Optimizer | AdamW + Cosine LR + Warmup |\n",
    "| Target | >75% test accuracy |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 1 ‚Äî Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí T4 GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 2 ‚Äî Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.benchmark = True   # Faster convolutions\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "\n",
    "print('‚úì All imports successful')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 3 ‚Äî Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG ‚Äî adjust these to experiment\n",
    "# ============================================================\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'image_size':      32,\n",
    "    'patch_size':      4,\n",
    "    'num_classes':     10,\n",
    "    'embed_dim':       192,\n",
    "    'depth':           12,\n",
    "    'num_heads':       3,\n",
    "    'mlp_ratio':       4.0,\n",
    "    'dropout':         0.1,\n",
    "    'attn_dropout':    0.0,\n",
    "\n",
    "    # Training\n",
    "    'num_epochs':      100,\n",
    "    'batch_size':      128,   # Doubled from CPU version ‚Äî GPU can handle it\n",
    "    'peak_lr':         1e-3,\n",
    "    'weight_decay':    0.05,\n",
    "    'warmup_epochs':   10,\n",
    "    'label_smoothing': 0.1,\n",
    "    'grad_clip':       1.0,\n",
    "\n",
    "    # Data augmentation (stronger than CPU version)\n",
    "    'use_mixup':       True,\n",
    "    'mixup_alpha':     0.2,\n",
    "    'use_cutmix':      True,\n",
    "    'cutmix_alpha':    1.0,\n",
    "\n",
    "    # Checkpointing\n",
    "    'save_dir':        'checkpoints',\n",
    "    'resume_from':     None,   # Set to checkpoint path to resume, e.g. 'checkpoints/best.pth'\n",
    "}\n",
    "\n",
    "# Derived\n",
    "CONFIG['num_patches'] = (CONFIG['image_size'] // CONFIG['patch_size']) ** 2\n",
    "\n",
    "os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "\n",
    "print('Configuration:')\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k:<20} {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Step 4 ‚Äî Data Loading with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Training augmentation ‚Äî aggressive for ViT (data-hungry model)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.TrivialAugmentWide(),          # State-of-the-art augmentation policy\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "    transforms.RandomErasing(p=0.25),         # Helps ViT learn global features\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,  download=True, transform=train_transform)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True, num_workers=2, pin_memory=True,\n",
    "    persistent_workers=False   # Changed for kaggle enviornment\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CONFIG['batch_size'] * 2,\n",
    "    shuffle=False, num_workers=2, pin_memory=True,\n",
    "    persistent_workers=False   # Changed for kaggle enviornment\n",
    ")\n",
    "\n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(f'Train samples: {len(train_dataset):,}')\n",
    "print(f'Test  samples: {len(test_dataset):,}')\n",
    "print(f'Train batches: {len(train_loader):,}  (batch={CONFIG[\"batch_size\"]})')\n",
    "print(f'Test  batches: {len(test_loader):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 5 ‚Äî ViT-Tiny Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and project to embedding dim.\"\"\"\n",
    "    def __init__(self, image_size, patch_size, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        # Conv2d as a patch extractor ‚Äî equivalent to linear projection but faster\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)           # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2)           # (B, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)      # (B, num_patches, embed_dim)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"Efficient multi-head self-attention with optional flash attention.\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = embed_dim // num_heads\n",
    "        self.scale     = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv  = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)   # (3, B, heads, N, head_dim)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Use PyTorch scaled_dot_product_attention if available (Flash Attention)\n",
    "        if hasattr(F, 'scaled_dot_product_attention'):\n",
    "            x = F.scaled_dot_product_attention(q, k, v, dropout_p=self.attn_drop.p if self.training else 0.0)\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Feed-forward block with GELU activation.\"\"\"\n",
    "    def __init__(self, embed_dim, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(embed_dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Pre-norm transformer block (more stable than post-norm).\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0, attn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = MultiHeadSelfAttention(embed_dim, num_heads, attn_dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp   = MLP(embed_dim, mlp_ratio, dropout)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop(self.attn(self.norm1(x)))   # Residual connection\n",
    "        x = x + self.drop(self.mlp(self.norm2(x)))    # Residual connection\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTTiny(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer - Tiny variant.\n",
    "    Based on: Dosovitskiy et al. 2020 (https://arxiv.org/abs/2010.11929)\n",
    "    Config matches DeiT-Tiny: embed_dim=192, depth=12, heads=3\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            cfg['image_size'], cfg['patch_size'], embed_dim=cfg['embed_dim']\n",
    "        )\n",
    "        num_patches = cfg['num_patches']\n",
    "\n",
    "        # Learnable CLS token and positional embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, cfg['embed_dim']))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, cfg['embed_dim']))\n",
    "        self.pos_drop  = nn.Dropout(cfg['dropout'])\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                cfg['embed_dim'], cfg['num_heads'], cfg['mlp_ratio'],\n",
    "                cfg['dropout'], cfg['attn_dropout']\n",
    "            )\n",
    "            for _ in range(cfg['depth'])\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(cfg['embed_dim'])\n",
    "        self.head = nn.Linear(cfg['embed_dim'], cfg['num_classes'])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Follows original ViT initialisation\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)                                          # (B, N, D)\n",
    "        cls = self.cls_token.expand(B, -1, -1)                          # (B, 1, D)\n",
    "        x   = torch.cat([cls, x], dim=1)                                # (B, N+1, D)\n",
    "        x   = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])                                        # CLS token ‚Üí logits\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = ViTTiny(CONFIG).to(device)\n",
    "\n",
    "total_params     = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total parameters:     {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Step 6 ‚Äî MixUp / CutMix Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"MixUp: Zhang et al. 2018 (https://arxiv.org/abs/1710.09412)\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
    "    idx = torch.randperm(x.size(0), device=x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[idx]\n",
    "    return mixed_x, y, y[idx], lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    \"\"\"CutMix: Yun et al. 2019 (https://arxiv.org/abs/1905.04899)\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    idx = torch.randperm(x.size(0), device=x.device)\n",
    "    _, _, H, W = x.shape\n",
    "\n",
    "    cut_ratio = math.sqrt(1 - lam)\n",
    "    cut_h, cut_w = int(H * cut_ratio), int(W * cut_ratio)\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    x1, x2 = max(cx - cut_w // 2, 0), min(cx + cut_w // 2, W)\n",
    "    y1, y2 = max(cy - cut_h // 2, 0), min(cy + cut_h // 2, H)\n",
    "\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
    "    lam = 1 - (y2 - y1) * (x2 - x1) / (H * W)\n",
    "    return mixed_x, y, y[idx], lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "print('‚úì MixUp and CutMix ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Step 7 ‚Äî Optimizer, Scheduler & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate weight decay ‚Äî don't apply to bias, LayerNorm, CLS token, pos embed\n",
    "decay_params     = [p for n, p in model.named_parameters() if p.requires_grad and\n",
    "                    not any(nd in n for nd in ['bias', 'norm', 'cls_token', 'pos_embed'])]\n",
    "no_decay_params  = [p for n, p in model.named_parameters() if p.requires_grad and\n",
    "                    any(nd in n for nd in ['bias', 'norm', 'cls_token', 'pos_embed'])]\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': decay_params,    'weight_decay': CONFIG['weight_decay']},\n",
    "    {'params': no_decay_params, 'weight_decay': 0.0},\n",
    "], lr=CONFIG['peak_lr'], betas=(0.9, 0.999))\n",
    "\n",
    "# Linear warmup + cosine decay\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < CONFIG['warmup_epochs']:\n",
    "        return (epoch + 1) / CONFIG['warmup_epochs']\n",
    "    progress = (epoch - CONFIG['warmup_epochs']) / (CONFIG['num_epochs'] - CONFIG['warmup_epochs'])\n",
    "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "# Mixed precision scaler ‚Äî major speedup on GPU\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=torch.cuda.is_available())\n",
    "\n",
    "print('‚úì Optimizer: AdamW')\n",
    "print('‚úì Scheduler: Linear warmup + Cosine decay')\n",
    "print('‚úì Loss: CrossEntropy with label smoothing')\n",
    "print(f'‚úì Mixed precision (AMP): {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 8 ‚Äî Checkpoint Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, path):\n",
    "    torch.save(state, path)\n",
    "    print(f'  ‚úì Checkpoint saved ‚Üí {path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(path, model, optimizer, scheduler, scaler):\n",
    "    ckpt = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state'])\n",
    "    scheduler.load_state_dict(ckpt['scheduler_state'])\n",
    "    if 'scaler_state' in ckpt:\n",
    "        scaler.load_state_dict(ckpt['scaler_state'])\n",
    "    print(f'‚úì Resumed from epoch {ckpt[\"epoch\"]} | Best acc: {ckpt[\"best_acc\"]:.2f}%')\n",
    "    return ckpt['epoch'], ckpt['best_acc'], ckpt['history']\n",
    "\n",
    "\n",
    "# Resume from checkpoint if specified\n",
    "start_epoch = 0\n",
    "best_acc    = 0.0\n",
    "history     = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'lr': []}\n",
    "\n",
    "if CONFIG['resume_from'] and os.path.exists(CONFIG['resume_from']):\n",
    "    start_epoch, best_acc, history = load_checkpoint(\n",
    "        CONFIG['resume_from'], model, optimizer, scheduler, scaler\n",
    "    )\n",
    "else:\n",
    "    print('Starting fresh training run')\n",
    "\n",
    "print(f'Starting from epoch: {start_epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 9 ‚Äî Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, scaler, epoch):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    use_mixup   = CONFIG['use_mixup']   and np.random.random() > 0.5\n",
    "    use_cutmix  = CONFIG['use_cutmix']  and np.random.random() > 0.5\n",
    "\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Apply MixUp or CutMix (not both at once)\n",
    "        mixed = False\n",
    "        if use_cutmix and np.random.random() > 0.5:\n",
    "            images, y_a, y_b, lam = cutmix_data(images, labels, CONFIG['cutmix_alpha'])\n",
    "            mixed = True\n",
    "        elif use_mixup:\n",
    "            images, y_a, y_b, lam = mixup_data(images, labels, CONFIG['mixup_alpha'])\n",
    "            mixed = True\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)  # Faster than zero_grad()\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "            logits = model(images)\n",
    "            if mixed:\n",
    "                loss = mixup_criterion(criterion, logits, y_a, y_b, lam)\n",
    "            else:\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()  # Accuracy on original labels\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'  [{i+1:>4}/{len(loader)}] Loss: {loss.item():.4f} | Acc: {100*correct/total:.2f}%', end='\\r')\n",
    "\n",
    "    return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n",
    "            logits = model(images)\n",
    "            loss   = criterion(logits, labels)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        correct    += logits.argmax(1).eq(labels).sum().item()\n",
    "        total      += labels.size(0)\n",
    "\n",
    "    return total_loss / total, 100.0 * correct / total\n",
    "\n",
    "\n",
    "print('‚úì Training functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 70)\n",
    "print(f'TRAINING ViT-Tiny on CIFAR-10  |  {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')\n",
    "print(f'Epochs: {start_epoch} ‚Üí {CONFIG[\"num_epochs\"]}  |  Batch: {CONFIG[\"batch_size\"]}  |  AMP: {torch.cuda.is_available()}')\n",
    "print('=' * 70)\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, CONFIG['num_epochs']):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, scaler, epoch)\n",
    "    test_loss,  test_acc  = evaluate(model, test_loader, criterion)\n",
    "    scheduler.step()\n",
    "\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['lr'].append(lr)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    is_best    = test_acc > best_acc\n",
    "    if is_best:\n",
    "        best_acc = test_acc\n",
    "\n",
    "    print(f'\\nEpoch [{epoch+1:>3}/{CONFIG[\"num_epochs\"]}]  '\n",
    "          f'Train: {train_loss:.4f} / {train_acc:.2f}%  |  '\n",
    "          f'Test: {test_loss:.4f} / {test_acc:.2f}%  |  '\n",
    "          f'LR: {lr:.6f}  |  {epoch_time:.1f}s  '\n",
    "          f'{\"‚òÖ BEST\" if is_best else \"\"}')\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if is_best:\n",
    "        save_checkpoint({\n",
    "            'epoch':           epoch + 1,\n",
    "            'model_state':     model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'scaler_state':    scaler.state_dict(),\n",
    "            'best_acc':        best_acc,\n",
    "            'history':         history,\n",
    "            'config':          CONFIG,\n",
    "        }, f'{CONFIG[\"save_dir\"]}/best_vit_cifar10.pth')\n",
    "\n",
    "    # Save periodic checkpoint every 10 epochs (safety net for Colab disconnects)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_checkpoint({\n",
    "            'epoch':           epoch + 1,\n",
    "            'model_state':     model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scheduler_state': scheduler.state_dict(),\n",
    "            'scaler_state':    scaler.state_dict(),\n",
    "            'best_acc':        best_acc,\n",
    "            'history':         history,\n",
    "            'config':          CONFIG,\n",
    "        }, f'{CONFIG[\"save_dir\"]}/checkpoint_epoch{epoch+1}.pth')\n",
    "\n",
    "total_time = time.time() - training_start\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'Training complete in {total_time/3600:.2f} hours')\n",
    "print(f'Best test accuracy: {best_acc:.2f}%')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 10 ‚Äî Results & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs  = gridspec.GridSpec(2, 3, figure=fig)\n",
    "\n",
    "epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss curves\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(epochs, history['train_loss'], label='Train', color='steelblue')\n",
    "ax1.plot(epochs, history['test_loss'],  label='Test',  color='coral')\n",
    "ax1.set_title('Loss'); ax1.set_xlabel('Epoch'); ax1.legend(); ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(epochs, history['train_acc'], label='Train', color='steelblue')\n",
    "ax2.plot(epochs, history['test_acc'],  label='Test',  color='coral')\n",
    "ax2.axhline(best_acc, linestyle='--', color='green', alpha=0.7, label=f'Best {best_acc:.2f}%')\n",
    "ax2.set_title('Accuracy (%)'); ax2.set_xlabel('Epoch'); ax2.legend(); ax2.grid(alpha=0.3)\n",
    "\n",
    "# LR schedule\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(epochs, history['lr'], color='purple')\n",
    "ax3.set_title('Learning Rate'); ax3.set_xlabel('Epoch'); ax3.grid(alpha=0.3)\n",
    "\n",
    "# Per-class accuracy\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "model.eval()\n",
    "class_correct = [0] * 10\n",
    "class_total   = [0] * 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        preds  = model(images).argmax(1).cpu()\n",
    "        for pred, label in zip(preds, labels):\n",
    "            class_total[label]   += 1\n",
    "            class_correct[label] += (pred == label).item()\n",
    "\n",
    "class_acc = [100 * class_correct[i] / class_total[i] for i in range(10)]\n",
    "colors    = ['green' if a >= 70 else 'orange' if a >= 55 else 'red' for a in class_acc]\n",
    "bars = ax4.bar(CLASSES, class_acc, color=colors, edgecolor='white')\n",
    "ax4.axhline(np.mean(class_acc), linestyle='--', color='black', alpha=0.5, label=f'Mean {np.mean(class_acc):.1f}%')\n",
    "ax4.set_title('Per-Class Test Accuracy'); ax4.set_ylabel('Accuracy (%)'); ax4.legend(); ax4.grid(alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars, class_acc):\n",
    "    ax4.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5, f'{acc:.1f}%', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle(f'ViT-Tiny CIFAR-10 | Best Accuracy: {best_acc:.2f}%', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Plot saved as vit_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 11 ‚Äî Download Checkpoint to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download best checkpoint and results plot\n",
    "from google.colab import files\n",
    "\n",
    "best_ckpt = f'{CONFIG[\"save_dir\"]}/best_vit_cifar10.pth'\n",
    "if os.path.exists(best_ckpt):\n",
    "    print(f'Downloading checkpoint ({os.path.getsize(best_ckpt)/1e6:.1f} MB)...')\n",
    "    files.download(best_ckpt)\n",
    "else:\n",
    "    print('No checkpoint found ‚Äî training may not have completed')\n",
    "\n",
    "files.download('vit_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Step 12 ‚Äî Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('FINAL RESULTS SUMMARY')\n",
    "print('=' * 60)\n",
    "print(f'Model:              ViT-Tiny')\n",
    "print(f'Parameters:         {total_params:,}')\n",
    "print(f'Dataset:            CIFAR-10')\n",
    "print(f'Epochs trained:     {len(history[\"train_acc\"])}')\n",
    "print(f'Best test accuracy: {best_acc:.2f}%')\n",
    "print(f'Final train acc:    {history[\"train_acc\"][-1]:.2f}%')\n",
    "print(f'Final test acc:     {history[\"test_acc\"][-1]:.2f}%')\n",
    "print()\n",
    "print('Per-class accuracy:')\n",
    "for cls, acc in zip(CLASSES, class_acc):\n",
    "    bar = '‚ñà' * int(acc // 5)\n",
    "    print(f'  {cls:<12} {acc:5.1f}%  {bar}')\n",
    "print('=' * 60)"
   ]
  }
 ]
}
