{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5,\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"3.10.0\"\n",
    "  }\n",
    " },\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"# üîç ViT-Tiny ‚Äî Attention Map Visualisation\\n**Loads the trained checkpoint and visualises what the model attends to.**\\n\\nThree outputs per image:\\n1. **Per-head attention** ‚Äî what each of the 3 heads focuses on in the last layer\\n2. **All-layers CLS attention** ‚Äî how attention evolves layer by layer\\n3. **Attention rollout** ‚Äî propagated attention through all 12 layers (Abnar & Zuidema, 2020)\",\n",
    "   \"id\": \"1af829c8c3dbb0dc\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## ‚úÖ Step 1 ‚Äî Verify GPU\",\n",
    "   \"id\": \"e3acb9b6de4dc038\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"import torch\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nprint(f'Device: {device}')\\nif torch.cuda.is_available():\\n    print(f'GPU:  {torch.cuda.get_device_name(0)}')\\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\",\n",
    "   \"id\": \"82e1cef37916e8d6\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üì¶ Step 2 ‚Äî Imports\",\n",
    "   \"id\": \"27f0c939d8b398b5\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"from torchvision import datasets, transforms\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('‚úì All imports successful')\\n\",\n",
    "    \"print(f'PyTorch version: {torch.__version__}')\"\n",
    "   ],\n",
    "   \"id\": \"33645192b45ca818\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## ‚öôÔ∏è Step 3 ‚Äî Config\",\n",
    "   \"id\": \"b9a4c7064fd49668\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ‚îÄ‚îÄ Visualisation settings ‚Äî change these ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\",\n",
    "    \"CHECKPOINT_PATH = '/kaggle/working/checkpoints/best.pth'\\n\",\n",
    "    \"IMAGE_IDX       = 0      # index into CIFAR-10 test set (0‚Äì9999)\\n\",\n",
    "    \"LAYER           = 11     # layer for per-head plot, 0-indexed (11 = last)\\n\",\n",
    "    \"SAVE_DIR        = '/kaggle/working/attention_maps'\\n\",\n",
    "    \"\\n\",\n",
    "    \"# ‚îÄ‚îÄ Must match training config exactly ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n\",\n",
    "    \"CONFIG = {\\n\",\n",
    "    \"    'image_size':   32,\\n\",\n",
    "    \"    'patch_size':   4,\\n\",\n",
    "    \"    'num_classes':  10,\\n\",\n",
    "    \"    'embed_dim':    192,\\n\",\n",
    "    \"    'depth':        12,\\n\",\n",
    "    \"    'num_heads':    3,\\n\",\n",
    "    \"    'mlp_ratio':    4.0,\\n\",\n",
    "    \"    'dropout':      0.1,\\n\",\n",
    "    \"    'attn_dropout': 0.0,\\n\",\n",
    "    \"}\\n\",\n",
    "    \"CONFIG['num_patches'] = (CONFIG['image_size'] // CONFIG['patch_size']) ** 2\\n\",\n",
    "    \"\\n\",\n",
    "    \"CIFAR_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\\n\",\n",
    "    \"                 'dog', 'frog', 'horse', 'ship', 'truck']\\n\",\n",
    "    \"CIFAR10_MEAN  = (0.4914, 0.4822, 0.4465)\\n\",\n",
    "    \"CIFAR10_STD   = (0.2470, 0.2435, 0.2616)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Precomputed numpy arrays for denorm ‚Äî no allocation per call\\n\",\n",
    "    \"_MEAN_NP = np.array(CIFAR10_MEAN, dtype=np.float32)\\n\",\n",
    "    \"_STD_NP  = np.array(CIFAR10_STD,  dtype=np.float32)\\n\",\n",
    "    \"\\n\",\n",
    "    \"os.makedirs(SAVE_DIR, exist_ok=True)\\n\",\n",
    "    \"print('Config OK')\\n\",\n",
    "    \"print(f'Checkpoint: {CHECKPOINT_PATH}')\\n\",\n",
    "    \"print(f'Image idx:  {IMAGE_IDX}')\\n\",\n",
    "    \"print(f'Layer:      {LAYER} (0-indexed)')\\n\",\n",
    "    \"\\n\",\n",
    "    \"if not 0 <= LAYER < CONFIG['depth']:\\n\",\n",
    "    \"    raise ValueError(f'LAYER must be in [0, {CONFIG[\\\"depth\\\"] - 1}], got {LAYER}')\"\n",
    "   ],\n",
    "   \"id\": \"f67343592d4364a0\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üèóÔ∏è Step 4 ‚Äî ViT-Tiny Architecture (with attention extraction)\",\n",
    "   \"id\": \"db7fd5f7c18d82ed\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class PatchEmbedding(nn.Module):\\n\",\n",
    "    \"    def __init__(self, image_size, patch_size, in_channels=3, embed_dim=192):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.num_patches = (image_size // patch_size) ** 2\\n\",\n",
    "    \"        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\\n\",\n",
    "    \"        self.norm = nn.LayerNorm(embed_dim)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        x = self.proj(x).flatten(2).transpose(1, 2)\\n\",\n",
    "    \"        return self.norm(x)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class MultiHeadSelfAttention(nn.Module):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Identical to training notebook, with one addition:\\n\",\n",
    "    \"    when return_attn=True, returns (output, attn_weights) where\\n\",\n",
    "    \"    attn_weights is (B, num_heads, N, N).\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    def __init__(self, embed_dim, num_heads, attn_dropout=0.0):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        assert embed_dim % num_heads == 0\\n\",\n",
    "    \"        self.num_heads = num_heads\\n\",\n",
    "    \"        self.head_dim  = embed_dim // num_heads\\n\",\n",
    "    \"        self.scale     = self.head_dim ** -0.5\\n\",\n",
    "    \"        self.qkv       = nn.Linear(embed_dim, embed_dim * 3, bias=False)\\n\",\n",
    "    \"        self.proj      = nn.Linear(embed_dim, embed_dim)\\n\",\n",
    "    \"        self.attn_drop = nn.Dropout(attn_dropout)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x, return_attn=False):\\n\",\n",
    "    \"        B, N, C = x.shape\\n\",\n",
    "    \"        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\\n\",\n",
    "    \"        q, k, v = qkv.unbind(0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Manual scaled dot-product (flash attention doesn't return weights)\\n\",\n",
    "    \"        attn = (q @ k.transpose(-2, -1)) * self.scale\\n\",\n",
    "    \"        attn = attn.softmax(dim=-1)\\n\",\n",
    "    \"        attn = self.attn_drop(attn)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\\n\",\n",
    "    \"        out = self.proj(out)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if return_attn:\\n\",\n",
    "    \"            return out, attn.detach()\\n\",\n",
    "    \"        return out\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class MLP(nn.Module):\\n\",\n",
    "    \"    def __init__(self, embed_dim, mlp_ratio=4.0, dropout=0.0):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        hidden = int(embed_dim * mlp_ratio)\\n\",\n",
    "    \"        self.net = nn.Sequential(\\n\",\n",
    "    \"            nn.Linear(embed_dim, hidden), nn.GELU(), nn.Dropout(dropout),\\n\",\n",
    "    \"            nn.Linear(hidden, embed_dim), nn.Dropout(dropout),\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        return self.net(x)\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class TransformerBlock(nn.Module):\\n\",\n",
    "    \"    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0, attn_dropout=0.0):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.norm1 = nn.LayerNorm(embed_dim)\\n\",\n",
    "    \"        self.attn  = MultiHeadSelfAttention(embed_dim, num_heads, attn_dropout)\\n\",\n",
    "    \"        self.norm2 = nn.LayerNorm(embed_dim)\\n\",\n",
    "    \"        self.mlp   = MLP(embed_dim, mlp_ratio, dropout)\\n\",\n",
    "    \"        self.drop  = nn.Dropout(dropout)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x, return_attn=False):\\n\",\n",
    "    \"        attn_out = self.attn(self.norm1(x), return_attn=return_attn)\\n\",\n",
    "    \"        if return_attn:\\n\",\n",
    "    \"            attn_out, weights = attn_out\\n\",\n",
    "    \"            x = x + self.drop(attn_out)\\n\",\n",
    "    \"            x = x + self.drop(self.mlp(self.norm2(x)))\\n\",\n",
    "    \"            return x, weights\\n\",\n",
    "    \"        x = x + self.drop(attn_out)\\n\",\n",
    "    \"        x = x + self.drop(self.mlp(self.norm2(x)))\\n\",\n",
    "    \"        return x\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class ViTTiny(nn.Module):\\n\",\n",
    "    \"    def __init__(self, cfg):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.patch_embed = PatchEmbedding(cfg['image_size'], cfg['patch_size'], embed_dim=cfg['embed_dim'])\\n\",\n",
    "    \"        self.cls_token   = nn.Parameter(torch.zeros(1, 1, cfg['embed_dim']))\\n\",\n",
    "    \"        self.pos_embed   = nn.Parameter(torch.zeros(1, cfg['num_patches'] + 1, cfg['embed_dim']))\\n\",\n",
    "    \"        self.pos_drop    = nn.Dropout(cfg['dropout'])\\n\",\n",
    "    \"        self.blocks      = nn.ModuleList([\\n\",\n",
    "    \"            TransformerBlock(cfg['embed_dim'], cfg['num_heads'], cfg['mlp_ratio'],\\n\",\n",
    "    \"                             cfg['dropout'], cfg['attn_dropout'])\\n\",\n",
    "    \"            for _ in range(cfg['depth'])\\n\",\n",
    "    \"        ])\\n\",\n",
    "    \"        self.norm = nn.LayerNorm(cfg['embed_dim'])\\n\",\n",
    "    \"        self.head = nn.Linear(cfg['embed_dim'], cfg['num_classes'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def _init_weights(self):\\n\",\n",
    "    \"        nn.init.trunc_normal_(self.pos_embed, std=0.02)\\n\",\n",
    "    \"        nn.init.trunc_normal_(self.cls_token, std=0.02)\\n\",\n",
    "    \"        for m in self.modules():\\n\",\n",
    "    \"            if isinstance(m, nn.Linear):\\n\",\n",
    "    \"                nn.init.trunc_normal_(m.weight, std=0.02)\\n\",\n",
    "    \"                if m.bias is not None: nn.init.zeros_(m.bias)\\n\",\n",
    "    \"            elif isinstance(m, nn.LayerNorm):\\n\",\n",
    "    \"                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        Always captures attention from every layer.\\n\",\n",
    "    \"        Returns: (logits, list of attn_weights), each attn_weights is (B, heads, N, N) on CPU.\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        B = x.shape[0]\\n\",\n",
    "    \"        x   = self.patch_embed(x)\\n\",\n",
    "    \"        cls = self.cls_token.expand(B, -1, -1)\\n\",\n",
    "    \"        x   = self.pos_drop(torch.cat([cls, x], dim=1) + self.pos_embed)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        all_attn = []\\n\",\n",
    "    \"        for block in self.blocks:\\n\",\n",
    "    \"            x, w = block(x, return_attn=True)\\n\",\n",
    "    \"            all_attn.append(w)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        x = self.norm(x)\\n\",\n",
    "    \"        logits = self.head(x[:, 0])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        # Single bulk transfer after all layers are done\\n\",\n",
    "    \"        all_attn = [w.cpu() for w in all_attn]\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return logits, all_attn\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"print('‚úì Architecture defined')\"\n",
    "   ],\n",
    "   \"id\": \"bdc56552b050f1f6\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üíæ Step 5 ‚Äî Load Checkpoint\",\n",
    "   \"id\": \"4d09e6142bc1cf97\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"model = ViTTiny(CONFIG).to(device)\\n\\n# Kaggle checkpoint uses 'model_state' and 'best_acc' keys\\nckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\\nmodel.load_state_dict(ckpt['model_state'])\\nmodel.eval()\\n\\nprint(f'‚úì Loaded checkpoint')\\nprint(f'  Epoch:    {ckpt[\\\"epoch\\\"]}')\\nprint(f'  Best acc: {ckpt[\\\"best_acc\\\"]:.2f}%')\\nprint(f'  Params:   {sum(p.numel() for p in model.parameters()):,}')\",\n",
    "   \"id\": \"5b97825503bad473\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üñºÔ∏è Step 6 ‚Äî Load Test Image\",\n",
    "   \"id\": \"b0570145aafebd9e\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"test_transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.ToTensor(),\\n\",\n",
    "    \"    transforms.Normalize(mean=CIFAR10_MEAN, std=CIFAR10_STD),\\n\",\n",
    "    \"])\\n\",\n",
    "    \"test_dataset = datasets.CIFAR10(root='/kaggle/working/data', train=False, download=True, transform=test_transform)\\n\",\n",
    "    \"\\n\",\n",
    "    \"n_test = len(test_dataset)\\n\",\n",
    "    \"if not 0 <= IMAGE_IDX < n_test:\\n\",\n",
    "    \"    raise ValueError(f'IMAGE_IDX must be in [0, {n_test - 1}], got {IMAGE_IDX}')\\n\",\n",
    "    \"\\n\",\n",
    "    \"img_tensor, true_label = test_dataset[IMAGE_IDX]\\n\",\n",
    "    \"img_tensor = img_tensor.unsqueeze(0).to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'Image #{IMAGE_IDX} ‚Äî True class: {CIFAR_CLASSES[true_label]}')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Run forward pass\\n\",\n",
    "    \"with torch.no_grad():\\n\",\n",
    "    \"    logits, all_attn_weights = model(img_tensor)\\n\",\n",
    "    \"\\n\",\n",
    "    \"probs      = logits.softmax(dim=1)[0]\\n\",\n",
    "    \"pred_label = probs.argmax().item()\\n\",\n",
    "    \"confidence = probs[pred_label].item()\\n\",\n",
    "    \"correct    = '‚úì' if pred_label == true_label else '‚úó'\\n\",\n",
    "    \"print(f'Predicted:  {CIFAR_CLASSES[pred_label]} ({confidence:.1%}) {correct}')\"\n",
    "   ],\n",
    "   \"id\": \"35d05e9af1351662\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üõ†Ô∏è Step 7 ‚Äî Helper Functions\",\n",
    "   \"id\": \"cba3acc7f011c121\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"def denorm(tensor):\\n    \\\"\\\"\\\"(C,H,W) tensor ‚Üí (H,W,C) float32 numpy, clipped to [0,1].\\\"\\\"\\\"\\n    img = tensor.cpu().numpy().transpose(1, 2, 0)\\n    img = img * _STD_NP + _MEAN_NP\\n    return np.clip(img, 0.0, 1.0, out=img)\\n\\ndef _normalise_map(m):\\n    \\\"\\\"\\\"Min-max normalise 2D array to [0, 1].\\\"\\\"\\\"\\n    lo, hi = m.min(), m.max()\\n    return (m - lo) / (hi - lo + 1e-8)\\n\\ndef _grid(attn_weights):\\n    \\\"\\\"\\\"Patches per side derived from attention shape.\\\"\\\"\\\"\\n    return int((attn_weights.shape[-1] - 1) ** 0.5)\\n\\ndef _title(true_label, pred_label):\\n    mark = '‚úì' if true_label == pred_label else '‚úó'\\n    return f'True: {CIFAR_CLASSES[true_label]}  |  Predicted: {CIFAR_CLASSES[pred_label]} {mark}'\\n\\ndef attention_rollout(attn_weights_list):\\n    \\\"\\\"\\\"\\n    Attention Rollout (Abnar & Zuidema, 2020).\\n    Propagates CLS attention through all layers with residual correction.\\n    Returns (n_patches,) numpy array.\\n    \\\"\\\"\\\"\\n    N   = attn_weights_list[0].shape[-1]\\n    eye = np.eye(N, dtype=np.float32)\\n    rollout = eye.copy()\\n    for attn in attn_weights_list:\\n        attn_avg = attn[0].numpy().mean(axis=0)      # (N, N)\\n        attn_hat = 0.5 * attn_avg + 0.5 * eye\\n        attn_hat /= attn_hat.sum(axis=-1, keepdims=True)\\n        rollout = attn_hat @ rollout\\n    return rollout[0, 1:]   # CLS row, patch columns only\\n\\nprint('‚úì Helpers defined')\",\n",
    "   \"id\": \"2f7ae8dd561c1454\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üìä Plot 1 ‚Äî Per-Head CLS Attention (Last Layer)\",\n",
    "   \"id\": \"8aff36b8f7a8d7ed\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"def plot_single_layer_heads(img_tensor, attn_weights, layer_idx, true_label, pred_label, save_path=None):\\n    num_heads = attn_weights.shape[1]\\n    grid = _grid(attn_weights)\\n    img  = denorm(img_tensor[0])\\n\\n    fig, axes = plt.subplots(1, num_heads + 1, figsize=(4 * (num_heads + 1), 4))\\n    fig.suptitle(f'Layer {layer_idx + 1} ‚Äî Per-Head CLS Attention\\\\n{_title(true_label, pred_label)}',\\n                 fontsize=12, fontweight='bold')\\n\\n    axes[0].imshow(img); axes[0].set_title('Input', fontsize=10); axes[0].axis('off')\\n\\n    for h in range(num_heads):\\n        cls_attn = attn_weights[0, h, 0, 1:].numpy()\\n        attn_map = _normalise_map(cls_attn.reshape(grid, grid))\\n        ax = axes[h + 1]\\n        ax.imshow(img)\\n        hm = ax.imshow(attn_map, cmap='hot', alpha=0.6,\\n                       extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\\n        ax.set_title(f'Head {h + 1}', fontsize=10); ax.axis('off')\\n        plt.colorbar(hm, ax=ax, fraction=0.046, pad=0.04)\\n\\n    plt.tight_layout()\\n    if save_path:\\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\\n        print(f'Saved: {save_path}')\\n    plt.show()\\n    plt.close(fig)\\n\\n\\nplot_single_layer_heads(\\n    img_tensor, all_attn_weights[LAYER], LAYER, true_label, pred_label,\\n    save_path=f'{SAVE_DIR}/attn_layer{LAYER + 1}_heads.png'\\n)\",\n",
    "   \"id\": \"1631ea1272f20521\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üìä Plot 2 ‚Äî CLS Attention Across All Layers\",\n",
    "   \"id\": \"32cee139983c1ec2\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"def plot_all_layers_cls_attention(img_tensor, all_attn_weights, true_label, pred_label, save_path=None):\\n    num_layers = len(all_attn_weights)\\n    grid = _grid(all_attn_weights[0])\\n    img  = denorm(img_tensor[0])\\n\\n    cols = 4\\n    rows = (num_layers + 1 + cols - 1) // cols\\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.5, rows * 3.5))\\n    axes = axes.ravel()\\n\\n    fig.suptitle(f'CLS Attention ‚Äî All {num_layers} Layers (mean over heads)\\\\n{_title(true_label, pred_label)}',\\n                 fontsize=12, fontweight='bold')\\n\\n    axes[0].imshow(img); axes[0].set_title('Input', fontsize=9); axes[0].axis('off')\\n\\n    for i, aw in enumerate(all_attn_weights):\\n        cls_attn = aw[0].numpy().mean(axis=0)[0, 1:]\\n        attn_map = _normalise_map(cls_attn.reshape(grid, grid))\\n        ax = axes[i + 1]\\n        ax.imshow(img)\\n        ax.imshow(attn_map, cmap='hot', alpha=0.6,\\n                  extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\\n        ax.set_title(f'Layer {i + 1}', fontsize=9); ax.axis('off')\\n\\n    for i in range(num_layers + 1, len(axes)):\\n        axes[i].axis('off')\\n\\n    plt.tight_layout()\\n    if save_path:\\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\\n        print(f'Saved: {save_path}')\\n    plt.show()\\n    plt.close(fig)\\n\\n\\nplot_all_layers_cls_attention(\\n    img_tensor, all_attn_weights, true_label, pred_label,\\n    save_path=f'{SAVE_DIR}/attn_all_layers.png'\\n)\",\n",
    "   \"id\": \"207033bf9dc3ffc6\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üìä Plot 3 ‚Äî Attention Rollout\",\n",
    "   \"id\": \"97d028a3b3a4a118\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": \"def plot_attention_rollout(img_tensor, all_attn_weights, true_label, pred_label, save_path=None):\\n    grid = _grid(all_attn_weights[0])\\n    img  = denorm(img_tensor[0])\\n\\n    rollout  = attention_rollout(all_attn_weights)\\n    attn_map = _normalise_map(rollout.reshape(grid, grid))\\n\\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\\n    fig.suptitle(f'Attention Rollout\\\\n{_title(true_label, pred_label)}',\\n                 fontsize=12, fontweight='bold')\\n\\n    axes[0].imshow(img); axes[0].set_title('Input Image', fontsize=10); axes[0].axis('off')\\n\\n    hm = axes[1].imshow(attn_map, cmap='hot', interpolation='bilinear')\\n    axes[1].set_title(f'Rollout Map ({grid}√ó{grid} patches)', fontsize=10); axes[1].axis('off')\\n    plt.colorbar(hm, ax=axes[1], fraction=0.046, pad=0.04)\\n\\n    axes[2].imshow(img)\\n    axes[2].imshow(attn_map, cmap='hot', alpha=0.6,\\n                   extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\\n    axes[2].set_title('Overlay', fontsize=10); axes[2].axis('off')\\n\\n    plt.tight_layout()\\n    if save_path:\\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\\n        print(f'Saved: {save_path}')\\n    plt.show()\\n    plt.close(fig)\\n\\n\\nplot_attention_rollout(\\n    img_tensor, all_attn_weights, true_label, pred_label,\\n    save_path=f'{SAVE_DIR}/attn_rollout.png'\\n)\",\n",
    "   \"id\": \"42e2e599adc38209\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": \"## üíæ Step 8 ‚Äî Download Plots\",\n",
    "   \"id\": \"4d604e14316215c6\"\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import zipfile\\n\",\n",
    "    \"\\n\",\n",
    "    \"zip_path = '/kaggle/working/attention_maps.zip'\\n\",\n",
    "    \"with zipfile.ZipFile(zip_path, 'w') as zf:\\n\",\n",
    "    \"    for f in Path(SAVE_DIR).glob('*.png'):\\n\",\n",
    "    \"        zf.write(f, f.name)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f'‚úì Zipped all plots ‚Üí {zip_path}')\\n\",\n",
    "    \"print('Download via: Kaggle notebook ‚Üí Output tab ‚Üí attention_maps.zip')\"\n",
    "   ],\n",
    "   \"id\": \"727887ae37f2626c\"\n",
    "  }\n",
    " ]\n",
    "}\n"
   ],
   "id": "ff85f4200a11ff71"
  }
 ]
}
