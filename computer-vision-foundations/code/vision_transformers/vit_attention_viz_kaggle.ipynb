{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"kernelVersion","sourceId":298918667,"isSourceIdPinned":false}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1af829c8c3dbb0dc","cell_type":"markdown","source":"# üîç ViT-Tiny ‚Äî Attention Map Visualisation\n**Loads the trained checkpoint and visualises what the model attends to.**\n\nThree outputs per image:\n1. **Per-head attention** ‚Äî what each of the 3 heads focuses on in the last layer\n2. **All-layers CLS attention** ‚Äî how attention evolves layer by layer\n3. **Attention rollout** ‚Äî propagated attention through all 12 layers (Abnar & Zuidema, 2020)","metadata":{}},{"id":"e3acb9b6de4dc038","cell_type":"markdown","source":"## ‚úÖ Step 1 ‚Äî Verify GPU","metadata":{}},{"id":"82e1cef37916e8d6","cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif torch.cuda.is_available():\n    print(f'GPU:  {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:22:41.285660Z","iopub.execute_input":"2026-02-20T09:22:41.285987Z","iopub.status.idle":"2026-02-20T09:22:41.291238Z","shell.execute_reply.started":"2026-02-20T09:22:41.285930Z","shell.execute_reply":"2026-02-20T09:22:41.290479Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nGPU:  Tesla T4\nVRAM: 15.6 GB\n","output_type":"stream"}],"execution_count":8},{"id":"27f0c939d8b398b5","cell_type":"markdown","source":"## üì¶ Step 2 ‚Äî Imports","metadata":{}},{"id":"33645192b45ca818","cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\n\nprint('‚úì All imports successful')\nprint(f'PyTorch version: {torch.__version__}')\n\nimport zipfile\n\nzip_path = '/kaggle/working/attention_maps.zip'\nwith zipfile.ZipFile(zip_path, 'w') as zf:\n    for f in Path(SAVE_DIR).glob('*.png'):\n        zf.write(f, f.name)\n\nprint(f'‚úì Zipped all plots ‚Üí {zip_path}')\nprint('Download via: Kaggle notebook ‚Üí Output tab ‚Üí attention_maps.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:22:43.755747Z","iopub.execute_input":"2026-02-20T09:22:43.756216Z","iopub.status.idle":"2026-02-20T09:22:43.761179Z","shell.execute_reply.started":"2026-02-20T09:22:43.756180Z","shell.execute_reply":"2026-02-20T09:22:43.760366Z"}},"outputs":[{"name":"stdout","text":"‚úì All imports successful\nPyTorch version: 2.9.0+cu126\n","output_type":"stream"}],"execution_count":9},{"id":"b9a4c7064fd49668","cell_type":"markdown","source":"## ‚öôÔ∏è Step 3 ‚Äî Config","metadata":{}},{"id":"f67343592d4364a0","cell_type":"code","source":"# ‚îÄ‚îÄ Visualisation settings ‚Äî change these ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nCHECKPOINT_PATH = '/kaggle/input/checkpoints/best_vit_cifar10.pth'\nIMAGE_IDX       = 0      # index into CIFAR-10 test set (0‚Äì9999)\nLAYER           = 11     # layer for per-head plot, 0-indexed (11 = last)\nSAVE_DIR        = '/kaggle/working/attention_maps'\n\n# ‚îÄ‚îÄ Must match training config exactly ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nCONFIG = {\n    'image_size':   32,\n    'patch_size':   4,\n    'num_classes':  10,\n    'embed_dim':    192,\n    'depth':        12,\n    'num_heads':    3,\n    'mlp_ratio':    4.0,\n    'dropout':      0.1,\n    'attn_dropout': 0.0,\n}\nCONFIG['num_patches'] = (CONFIG['image_size'] // CONFIG['patch_size']) ** 2\n\nCIFAR_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n                 'dog', 'frog', 'horse', 'ship', 'truck']\nCIFAR10_MEAN  = (0.4914, 0.4822, 0.4465)\nCIFAR10_STD   = (0.2470, 0.2435, 0.2616)\n\n# Precomputed numpy arrays for denorm ‚Äî no allocation per call\n_MEAN_NP = np.array(CIFAR10_MEAN, dtype=np.float32)\n_STD_NP  = np.array(CIFAR10_STD,  dtype=np.float32)\n\nos.makedirs(SAVE_DIR, exist_ok=True)\nprint('Config OK')\nprint(f'Checkpoint: {CHECKPOINT_PATH}')\nprint(f'Image idx:  {IMAGE_IDX}')\nprint(f'Layer:      {LAYER} (0-indexed)')\n\nif not 0 <= LAYER < CONFIG['depth']:\n    raise ValueError(f'LAYER must be in [0, {CONFIG[\"depth\"] - 1}], got {LAYER}')\n\nckpt_path = Path(CHECKPOINT_PATH)\nif not ckpt_path.exists():\n    raise FileNotFoundError(\n        f\"Checkpoint not found: {CHECKPOINT_PATH}\\n\"\n        f\"  ‚Üí Attach the training notebook's output as a dataset input in Kaggle.\\n\"\n        f\"  ‚Üí Found these .pth files: {list(Path('/kaggle/input').rglob('*.pth'))}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:22:46.023077Z","iopub.execute_input":"2026-02-20T09:22:46.023720Z","iopub.status.idle":"2026-02-20T09:22:46.031158Z","shell.execute_reply.started":"2026-02-20T09:22:46.023691Z","shell.execute_reply":"2026-02-20T09:22:46.030512Z"}},"outputs":[{"name":"stdout","text":"Config OK\nCheckpoint: /kaggle/working/checkpoints/best.pth\nImage idx:  0\nLayer:      11 (0-indexed)\n","output_type":"stream"}],"execution_count":10},{"id":"db7fd5f7c18d82ed","cell_type":"markdown","source":"## üèóÔ∏è Step 4 ‚Äî ViT-Tiny Architecture (with attention extraction)","metadata":{}},{"id":"bdc56552b050f1f6","cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, image_size, patch_size, in_channels=3, embed_dim=192):\n        super().__init__()\n        self.num_patches = (image_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return self.norm(x)\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    \"\"\"\n    Identical to training notebook, with one addition:\n    when return_attn=True, returns (output, attn_weights) where\n    attn_weights is (B, num_heads, N, N).\n    \"\"\"\n    def __init__(self, embed_dim, num_heads, attn_dropout=0.0):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        self.num_heads = num_heads\n        self.head_dim  = embed_dim // num_heads\n        self.scale     = self.head_dim ** -0.5\n        self.qkv       = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n        self.proj      = nn.Linear(embed_dim, embed_dim)\n        self.attn_drop = nn.Dropout(attn_dropout)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv.unbind(0)\n\n        # Manual scaled dot-product (flash attention doesn't return weights)\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        out = self.proj(out)\n\n        if return_attn:\n            return out, attn.detach()\n        return out\n\n\nclass MLP(nn.Module):\n    def __init__(self, embed_dim, mlp_ratio=4.0, dropout=0.0):\n        super().__init__()\n        hidden = int(embed_dim * mlp_ratio)\n        self.net = nn.Sequential(\n            nn.Linear(embed_dim, hidden), nn.GELU(), nn.Dropout(dropout),\n            nn.Linear(hidden, embed_dim), nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0, attn_dropout=0.0):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn  = MultiHeadSelfAttention(embed_dim, num_heads, attn_dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp   = MLP(embed_dim, mlp_ratio, dropout)\n        self.drop  = nn.Dropout(dropout)\n\n    def forward(self, x):\n        attn_out, weights = self.attn(self.norm1(x), return_attn=True)\n        x = x + self.drop(attn_out)\n        x = x + self.drop(self.mlp(self.norm2(x)))\n        return x, weights\n\n\nclass ViTTiny(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(cfg['image_size'], cfg['patch_size'], embed_dim=cfg['embed_dim'])\n        self.cls_token   = nn.Parameter(torch.zeros(1, 1, cfg['embed_dim']))\n        self.pos_embed   = nn.Parameter(torch.zeros(1, cfg['num_patches'] + 1, cfg['embed_dim']))\n        self.pos_drop    = nn.Dropout(cfg['dropout'])\n        self.blocks      = nn.ModuleList([\n            TransformerBlock(cfg['embed_dim'], cfg['num_heads'], cfg['mlp_ratio'],\n                             cfg['dropout'], cfg['attn_dropout'])\n            for _ in range(cfg['depth'])\n        ])\n        self.norm = nn.LayerNorm(cfg['embed_dim'])\n        self.head = nn.Linear(cfg['embed_dim'], cfg['num_classes'])\n        \n\n    def _init_weights(self):\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None: nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LayerNorm):\n                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        \"\"\"\n        Always captures attention from every layer.\n        Returns: (logits, list of attn_weights), each attn_weights is (B, heads, N, N) on CPU.\n        \"\"\"\n        B = x.shape[0]\n        x   = self.patch_embed(x)\n        cls = self.cls_token.expand(B, -1, -1)\n        x   = self.pos_drop(torch.cat([cls, x], dim=1) + self.pos_embed)\n\n        all_attn = []\n        for block in self.blocks:\n            x, w = block(x, return_attn=True)\n            all_attn.append(w)\n\n        x = self.norm(x)\n        logits = self.head(x[:, 0])\n        \n        # Single bulk transfer after all layers are done\n        all_attn = [w.cpu() for w in all_attn]\n        \n        return logits, all_attn\n\n\nprint('‚úì Architecture defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.208149Z","iopub.execute_input":"2026-02-20T09:21:13.208436Z","iopub.status.idle":"2026-02-20T09:21:13.243731Z","shell.execute_reply.started":"2026-02-20T09:21:13.208407Z","shell.execute_reply":"2026-02-20T09:21:13.243041Z"}},"outputs":[{"name":"stdout","text":"‚úì Architecture defined\n","output_type":"stream"}],"execution_count":4},{"id":"4d09e6142bc1cf97","cell_type":"markdown","source":"## üíæ Step 5 ‚Äî Load Checkpoint","metadata":{}},{"id":"5b97825503bad473","cell_type":"code","source":"model = ViTTiny(CONFIG).to(device)\n\n# Kaggle checkpoint uses 'model_state' and 'best_acc' keys\nckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\nmodel.load_state_dict(ckpt['model_state'])\nmodel.eval()\n\nprint(f'‚úì Loaded checkpoint from {CHECKPOINT_PATH}')\nprint(f'  Keys:     {list(ckpt.keys())}')  # shows you exactly what's in the ckpt\nprint(f'  Epoch:    {ckpt.get(\"epoch\", \"N/A\")}')\nprint(f'  Best acc: {ckpt.get(\"best_acc\", float(\"nan\")):.2f}%')\nprint(f'  Params:   {sum(p.numel() for p in model.parameters()):,}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.244732Z","iopub.execute_input":"2026-02-20T09:21:13.245266Z","iopub.status.idle":"2026-02-20T09:21:13.620474Z","shell.execute_reply.started":"2026-02-20T09:21:13.245235Z","shell.execute_reply":"2026-02-20T09:21:13.619246Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2445114182.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Kaggle checkpoint uses 'model_state' and 'best_acc' keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/checkpoints/best.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/checkpoints/best.pth'","output_type":"error"}],"execution_count":5},{"id":"b0570145aafebd9e","cell_type":"markdown","source":"## üñºÔ∏è Step 6 ‚Äî Load Test Image","metadata":{}},{"id":"35d05e9af1351662","cell_type":"code","source":"test_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=CIFAR10_MEAN, std=CIFAR10_STD),\n])\ntest_dataset = datasets.CIFAR10(root='/kaggle/working/data', train=False, download=True, transform=test_transform)\n\nn_test = len(test_dataset)\nif not 0 <= IMAGE_IDX < n_test:\n    raise ValueError(f'IMAGE_IDX must be in [0, {n_test - 1}], got {IMAGE_IDX}')\n\nimg_tensor, true_label = test_dataset[IMAGE_IDX]\nimg_tensor = img_tensor.unsqueeze(0).to(device)\n\nprint(f'Image #{IMAGE_IDX} ‚Äî True class: {CIFAR_CLASSES[true_label]}')\n\n# Run forward pass\nwith torch.no_grad():\n    logits, all_attn_weights = model(img_tensor)\n\nprobs      = logits.softmax(dim=1)[0]\npred_label = probs.argmax().item()\nconfidence = probs[pred_label].item()\ncorrect    = '‚úì' if pred_label == true_label else '‚úó'\nprint(f'Predicted:  {CIFAR_CLASSES[pred_label]} ({confidence:.1%}) {correct}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.621012Z","iopub.status.idle":"2026-02-20T09:21:13.621293Z","shell.execute_reply.started":"2026-02-20T09:21:13.621158Z","shell.execute_reply":"2026-02-20T09:21:13.621173Z"}},"outputs":[],"execution_count":null},{"id":"cba3acc7f011c121","cell_type":"markdown","source":"## üõ†Ô∏è Step 7 ‚Äî Helper Functions","metadata":{}},{"id":"2f7ae8dd561c1454","cell_type":"code","source":"def denorm(tensor):\n    \"\"\"(C,H,W) tensor ‚Üí (H,W,C) float32 numpy, clipped to [0,1].\"\"\"\n    img = tensor.cpu().numpy().transpose(1, 2, 0)\n    img = img * _STD_NP + _MEAN_NP\n    return np.clip(img, 0.0, 1.0, out=img)\n\ndef _normalise_map(m):\n    \"\"\"Min-max normalise 2D array to [0, 1].\"\"\"\n    lo, hi = m.min(), m.max()\n    return (m - lo) / (hi - lo + 1e-8)\n\ndef _grid(attn_weights):\n    \"\"\"Patches per side derived from attention shape.\"\"\"\n    return int((attn_weights.shape[-1] - 1) ** 0.5)\n\ndef _title(true_label, pred_label):\n    mark = '‚úì' if true_label == pred_label else '‚úó'\n    return f'True: {CIFAR_CLASSES[true_label]}  |  Predicted: {CIFAR_CLASSES[pred_label]} {mark}'\n\ndef attention_rollout(attn_weights_list):\n    N   = attn_weights_list[0].shape[-1]\n    eye = np.eye(N, dtype=np.float32)\n    # Stack (L, B, H, N, N) ‚Üí index batch 0 ‚Üí (L, H, N, N), convert once\n    stacked = torch.stack(attn_weights_list)[:, 0].numpy()  # (L, H, N, N)\n    rollout = eye.copy()\n    for attn_avg in stacked.mean(axis=1):                   # (N, N) per layer\n        attn_hat  = 0.5 * attn_avg + 0.5 * eye\n        attn_hat /= attn_hat.sum(axis=-1, keepdims=True)\n        rollout   = attn_hat @ rollout\n    return rollout[0, 1:]   # CLS row, patch columns only\n\nprint('‚úì Helpers defined')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.622404Z","iopub.status.idle":"2026-02-20T09:21:13.623164Z","shell.execute_reply.started":"2026-02-20T09:21:13.622981Z","shell.execute_reply":"2026-02-20T09:21:13.623006Z"}},"outputs":[],"execution_count":null},{"id":"8aff36b8f7a8d7ed","cell_type":"markdown","source":"## üìä Plot 1 ‚Äî Per-Head CLS Attention (Last Layer)","metadata":{}},{"id":"1631ea1272f20521","cell_type":"code","source":"def plot_single_layer_heads(img_tensor, attn_weights, layer_idx, true_label, pred_label, save_path=None):\n    num_heads = attn_weights.shape[1]\n    grid = _grid(attn_weights)\n    img  = denorm(img_tensor[0])\n\n    fig, axes = plt.subplots(1, num_heads + 1, figsize=(4 * (num_heads + 1), 4))\n    fig.suptitle(f'Layer {layer_idx + 1} ‚Äî Per-Head CLS Attention\\n{_title(true_label, pred_label)}',\n                 fontsize=12, fontweight='bold')\n\n    axes[0].imshow(img); axes[0].set_title('Input', fontsize=10); axes[0].axis('off')\n\n    for h in range(num_heads):\n        cls_attn = attn_weights[0, h, 0, 1:].numpy()\n        attn_map = _normalise_map(cls_attn.reshape(grid, grid))\n        ax = axes[h + 1]\n        ax.imshow(img)\n        hm = ax.imshow(attn_map, cmap='hot', alpha=0.6,\n                       extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\n        ax.set_title(f'Head {h + 1}', fontsize=10); ax.axis('off')\n        plt.colorbar(hm, ax=ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f'Saved: {save_path}')\n    plt.show()\n    plt.close(fig)\n\n\nplot_single_layer_heads(\n    img_tensor, all_attn_weights[LAYER], LAYER, true_label, pred_label,\n    save_path=f'{SAVE_DIR}/attn_layer{LAYER + 1}_heads.png'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.624498Z","iopub.status.idle":"2026-02-20T09:21:13.625272Z","shell.execute_reply.started":"2026-02-20T09:21:13.625069Z","shell.execute_reply":"2026-02-20T09:21:13.625093Z"}},"outputs":[],"execution_count":null},{"id":"32cee139983c1ec2","cell_type":"markdown","source":"## üìä Plot 2 ‚Äî CLS Attention Across All Layers","metadata":{}},{"id":"207033bf9dc3ffc6","cell_type":"code","source":"def plot_all_layers_cls_attention(img_tensor, all_attn_weights, true_label, pred_label, save_path=None):\n    num_layers = len(all_attn_weights)\n    grid = _grid(all_attn_weights[0])\n    img  = denorm(img_tensor[0])\n\n    cols = 4\n    rows = (num_layers + 1 + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.5, rows * 3.5))\n    axes = axes.ravel()\n\n    fig.suptitle(f'CLS Attention ‚Äî All {num_layers} Layers (mean over heads)\\n{_title(true_label, pred_label)}',\n                 fontsize=12, fontweight='bold')\n\n    axes[0].imshow(img); axes[0].set_title('Input', fontsize=9); axes[0].axis('off')\n\n    for i, aw in enumerate(all_attn_weights):\n        cls_attn = aw[0].numpy().mean(axis=0)[0, 1:]\n        attn_map = _normalise_map(cls_attn.reshape(grid, grid))\n        ax = axes[i + 1]\n        ax.imshow(img)\n        ax.imshow(attn_map, cmap='hot', alpha=0.6,\n                  extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\n        ax.set_title(f'Layer {i + 1}', fontsize=9); ax.axis('off')\n\n    for i in range(num_layers + 1, len(axes)):\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f'Saved: {save_path}')\n    plt.show()\n    plt.close(fig)\n\n\nplot_all_layers_cls_attention(\n    img_tensor, all_attn_weights, true_label, pred_label,\n    save_path=f'{SAVE_DIR}/attn_all_layers.png'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.626245Z","iopub.status.idle":"2026-02-20T09:21:13.626647Z","shell.execute_reply.started":"2026-02-20T09:21:13.626434Z","shell.execute_reply":"2026-02-20T09:21:13.626454Z"}},"outputs":[],"execution_count":null},{"id":"97d028a3b3a4a118","cell_type":"markdown","source":"## üìä Plot 3 ‚Äî Attention Rollout","metadata":{}},{"id":"42e2e599adc38209","cell_type":"code","source":"def plot_attention_rollout(img_tensor, all_attn_weights, true_label, pred_label, save_path=None):\n    grid = _grid(all_attn_weights[0])\n    img  = denorm(img_tensor[0])\n\n    rollout  = attention_rollout(all_attn_weights)\n    attn_map = _normalise_map(rollout.reshape(grid, grid))\n\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    fig.suptitle(f'Attention Rollout\\n{_title(true_label, pred_label)}',\n                 fontsize=12, fontweight='bold')\n\n    axes[0].imshow(img); axes[0].set_title('Input Image', fontsize=10); axes[0].axis('off')\n\n    hm = axes[1].imshow(attn_map, cmap='hot', interpolation='bilinear')\n    axes[1].set_title(f'Rollout Map ({grid}√ó{grid} patches)', fontsize=10); axes[1].axis('off')\n    plt.colorbar(hm, ax=axes[1], fraction=0.046, pad=0.04)\n\n    axes[2].imshow(img)\n    axes[2].imshow(attn_map, cmap='hot', alpha=0.6,\n                   extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\n    axes[2].set_title('Overlay', fontsize=10); axes[2].axis('off')\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f'Saved: {save_path}')\n    plt.show()\n    plt.close(fig)\n\n\nplot_attention_rollout(\n    img_tensor, all_attn_weights, true_label, pred_label,\n    save_path=f'{SAVE_DIR}/attn_rollout.png'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.627795Z","iopub.status.idle":"2026-02-20T09:21:13.628037Z","shell.execute_reply.started":"2026-02-20T09:21:13.627921Z","shell.execute_reply":"2026-02-20T09:21:13.627936Z"}},"outputs":[],"execution_count":null},{"id":"4d604e14316215c6","cell_type":"markdown","source":"## üíæ Step 8 ‚Äî Download Plots","metadata":{}},{"id":"727887ae37f2626c","cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T09:21:13.628913Z","iopub.status.idle":"2026-02-20T09:21:13.629328Z","shell.execute_reply.started":"2026-02-20T09:21:13.629168Z","shell.execute_reply":"2026-02-20T09:21:13.629191Z"}},"outputs":[],"execution_count":null}]}