{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ” ViT-Tiny â€” Attention Map Visualisation\n**Loads the trained checkpoint and visualises what the model attends to.**\n\nThree outputs per image:\n1. **Per-head attention** â€” what each of the 3 heads focuses on in the last layer\n2. **All-layers CLS attention** â€” how attention evolves layer by layer\n3. **Attention rollout** â€” propagated attention through all 12 layers (Abnar & Zuidema, 2020)",
   "id": "1af829c8c3dbb0dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## âœ… Step 1 â€” Verify GPU",
   "id": "e3acb9b6de4dc038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Device: {device}')\nif torch.cuda.is_available():\n    print(f'GPU:  {torch.cuda.get_device_name(0)}')\n    print(f'VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')",
   "id": "82e1cef37916e8d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“¦ Step 2 â€” Imports",
   "id": "27f0c939d8b398b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print('âœ“ All imports successful')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ],
   "id": "33645192b45ca818"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## âš™ï¸ Step 3 â€” Config",
   "id": "b9a4c7064fd49668"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Visualisation settings â€” change these â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CHECKPOINT_PATH = '/kaggle/working/checkpoints/best.pth'\n",
    "IMAGE_IDX       = 0      # index into CIFAR-10 test set (0â€“9999)\n",
    "LAYER           = 11     # layer for per-head plot, 0-indexed (11 = last)\n",
    "SAVE_DIR        = '/kaggle/working/attention_maps'\n",
    "\n",
    "# â”€â”€ Must match training config exactly â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CONFIG = {\n",
    "    'image_size':   32,\n",
    "    'patch_size':   4,\n",
    "    'num_classes':  10,\n",
    "    'embed_dim':    192,\n",
    "    'depth':        12,\n",
    "    'num_heads':    3,\n",
    "    'mlp_ratio':    4.0,\n",
    "    'dropout':      0.1,\n",
    "    'attn_dropout': 0.0,\n",
    "}\n",
    "CONFIG['num_patches'] = (CONFIG['image_size'] // CONFIG['patch_size']) ** 2\n",
    "\n",
    "CIFAR_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "CIFAR10_MEAN  = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD   = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "# Precomputed numpy arrays for denorm â€” no allocation per call\n",
    "_MEAN_NP = np.array(CIFAR10_MEAN, dtype=np.float32)\n",
    "_STD_NP  = np.array(CIFAR10_STD,  dtype=np.float32)\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print('Config OK')\n",
    "print(f'Checkpoint: {CHECKPOINT_PATH}')\n",
    "print(f'Image idx:  {IMAGE_IDX}')\n",
    "print(f'Layer:      {LAYER} (0-indexed)')\n",
    "\n",
    "if not 0 <= LAYER < CONFIG['depth']:\n",
    "    raise ValueError(f'LAYER must be in [0, {CONFIG[\"depth\"] - 1}], got {LAYER}')"
   ],
   "id": "f67343592d4364a0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ—ï¸ Step 4 â€” ViT-Tiny Architecture (with attention extraction)",
   "id": "db7fd5f7c18d82ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels=3, embed_dim=192):\n",
    "        super().__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Identical to training notebook, with one addition:\n",
    "    when return_attn=True, returns (output, attn_weights) where\n",
    "    attn_weights is (B, num_heads, N, N).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, attn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = embed_dim // num_heads\n",
    "        self.scale     = self.head_dim ** -0.5\n",
    "        self.qkv       = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj      = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attn_drop = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Manual scaled dot-product (flash attention doesn't return weights)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        if return_attn:\n",
    "            return out, attn.detach().cpu()\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embed_dim, mlp_ratio=4.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(embed_dim * mlp_ratio)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, embed_dim), nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.0, attn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = MultiHeadSelfAttention(embed_dim, num_heads, attn_dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp   = MLP(embed_dim, mlp_ratio, dropout)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_attn=False):\n",
    "        attn_out = self.attn(self.norm1(x), return_attn=return_attn)\n",
    "        if return_attn:\n",
    "            attn_out, weights = attn_out\n",
    "            x = x + self.drop(attn_out)\n",
    "            x = x + self.drop(self.mlp(self.norm2(x)))\n",
    "            return x, weights\n",
    "        x = x + self.drop(attn_out)\n",
    "        x = x + self.drop(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTTiny(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(cfg['image_size'], cfg['patch_size'], embed_dim=cfg['embed_dim'])\n",
    "        self.cls_token   = nn.Parameter(torch.zeros(1, 1, cfg['embed_dim']))\n",
    "        self.pos_embed   = nn.Parameter(torch.zeros(1, cfg['num_patches'] + 1, cfg['embed_dim']))\n",
    "        self.pos_drop    = nn.Dropout(cfg['dropout'])\n",
    "        self.blocks      = nn.ModuleList([\n",
    "            TransformerBlock(cfg['embed_dim'], cfg['num_heads'], cfg['mlp_ratio'],\n",
    "                             cfg['dropout'], cfg['attn_dropout'])\n",
    "            for _ in range(cfg['depth'])\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(cfg['embed_dim'])\n",
    "        self.head = nn.Linear(cfg['embed_dim'], cfg['num_classes'])\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Always captures attention from every layer.\n",
    "        Returns: (logits, list of attn_weights), each attn_weights is (B, heads, N, N) on CPU.\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        x   = self.patch_embed(x)\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x   = self.pos_drop(torch.cat([cls, x], dim=1) + self.pos_embed)\n",
    "\n",
    "        all_attn = []\n",
    "        for block in self.blocks:\n",
    "            x, w = block(x, return_attn=True)\n",
    "            all_attn.append(w)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0]), all_attn\n",
    "\n",
    "\n",
    "print('âœ“ Architecture defined')"
   ],
   "id": "bdc56552b050f1f6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ’¾ Step 5 â€” Load Checkpoint",
   "id": "4d09e6142bc1cf97"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model = ViTTiny(CONFIG).to(device)\n\n# Kaggle checkpoint uses 'model_state' and 'best_acc' keys\nckpt = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\nmodel.load_state_dict(ckpt['model_state'])\nmodel.eval()\n\nprint(f'âœ“ Loaded checkpoint')\nprint(f'  Epoch:    {ckpt[\"epoch\"]}')\nprint(f'  Best acc: {ckpt[\"best_acc\"]:.2f}%')\nprint(f'  Params:   {sum(p.numel() for p in model.parameters()):,}')",
   "id": "5b97825503bad473"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ–¼ï¸ Step 6 â€” Load Test Image",
   "id": "b0570145aafebd9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=CIFAR10_MEAN, std=CIFAR10_STD),\n",
    "])\n",
    "test_dataset = datasets.CIFAR10(root='/kaggle/working/data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "n_test = len(test_dataset)\n",
    "if not 0 <= IMAGE_IDX < n_test:\n",
    "    raise ValueError(f'IMAGE_IDX must be in [0, {n_test - 1}], got {IMAGE_IDX}')\n",
    "\n",
    "img_tensor, true_label = test_dataset[IMAGE_IDX]\n",
    "img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "print(f'Image #{IMAGE_IDX} â€” True class: {CIFAR_CLASSES[true_label]}')\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    logits, all_attn_weights = model(img_tensor)\n",
    "\n",
    "probs      = logits.softmax(dim=1)[0]\n",
    "pred_label = probs.argmax().item()\n",
    "confidence = probs[pred_label].item()\n",
    "correct    = 'âœ“' if pred_label == true_label else 'âœ—'\n",
    "print(f'Predicted:  {CIFAR_CLASSES[pred_label]} ({confidence:.1%}) {correct}')"
   ],
   "id": "35d05e9af1351662"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ› ï¸ Step 7 â€” Helper Functions",
   "id": "cba3acc7f011c121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def denorm(tensor):\n    \"\"\"(C,H,W) tensor â†’ (H,W,C) float32 numpy, clipped to [0,1].\"\"\"\n    img = tensor.cpu().numpy().transpose(1, 2, 0)\n    img = img * _STD_NP + _MEAN_NP\n    return np.clip(img, 0.0, 1.0, out=img)\n\ndef _normalise_map(m):\n    \"\"\"Min-max normalise 2D array to [0, 1].\"\"\"\n    lo, hi = m.min(), m.max()\n    return (m - lo) / (hi - lo + 1e-8)\n\ndef _grid(attn_weights):\n    \"\"\"Patches per side derived from attention shape.\"\"\"\n    return int((attn_weights.shape[-1] - 1) ** 0.5)\n\ndef _title(true_label, pred_label):\n    mark = 'âœ“' if true_label == pred_label else 'âœ—'\n    return f'True: {CIFAR_CLASSES[true_label]}  |  Predicted: {CIFAR_CLASSES[pred_label]} {mark}'\n\ndef attention_rollout(attn_weights_list):\n    \"\"\"\n    Attention Rollout (Abnar & Zuidema, 2020).\n    Propagates CLS attention through all layers with residual correction.\n    Returns (n_patches,) numpy array.\n    \"\"\"\n    N   = attn_weights_list[0].shape[-1]\n    eye = np.eye(N, dtype=np.float32)\n    rollout = eye.copy()\n    for attn in attn_weights_list:\n        attn_avg = attn[0].numpy().mean(axis=0)      # (N, N)\n        attn_hat = 0.5 * attn_avg + 0.5 * eye\n        attn_hat /= attn_hat.sum(axis=-1, keepdims=True)\n        rollout = attn_hat @ rollout\n    return rollout[0, 1:]   # CLS row, patch columns only\n\nprint('âœ“ Helpers defined')",
   "id": "2f7ae8dd561c1454"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Plot 1 â€” Per-Head CLS Attention (Last Layer)",
   "id": "8aff36b8f7a8d7ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_single_layer_heads(img_tensor, attn_weights, layer_idx, true_label, pred_label, save_path=None):\n    num_heads = attn_weights.shape[1]\n    grid = _grid(attn_weights)\n    img  = denorm(img_tensor[0])\n\n    fig, axes = plt.subplots(1, num_heads + 1, figsize=(4 * (num_heads + 1), 4))\n    fig.suptitle(f'Layer {layer_idx + 1} â€” Per-Head CLS Attention\\n{_title(true_label, pred_label)}',\n                 fontsize=12, fontweight='bold')\n\n    axes[0].imshow(img); axes[0].set_title('Input', fontsize=10); axes[0].axis('off')\n\n    for h in range(num_heads):\n        cls_attn = attn_weights[0, h, 0, 1:].numpy()\n        attn_map = _normalise_map(cls_attn.reshape(grid, grid))\n        ax = axes[h + 1]\n        ax.imshow(img)\n        hm = ax.imshow(attn_map, cmap='hot', alpha=0.6,\n                       extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\n        ax.set_title(f'Head {h + 1}', fontsize=10); ax.axis('off')\n        plt.colorbar(hm, ax=ax, fraction=0.046, pad=0.04)\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f'Saved: {save_path}')\n    plt.show()\n    plt.close(fig)\n\n\nplot_single_layer_heads(\n    img_tensor, all_attn_weights[LAYER], LAYER, true_label, pred_label,\n    save_path=f'{SAVE_DIR}/attn_layer{LAYER + 1}_heads.png'\n)",
   "id": "1631ea1272f20521"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Plot 2 â€” CLS Attention Across All Layers",
   "id": "32cee139983c1ec2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_all_layers_cls_attention(img_tensor, all_attn_weights, true_label, pred_label, save_path=None):\n    num_layers = len(all_attn_weights)\n    grid = _grid(all_attn_weights[0])\n    img  = denorm(img_tensor[0])\n\n    cols = 4\n    rows = (num_layers + 1 + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 3.5, rows * 3.5))\n    axes = axes.ravel()\n\n    fig.suptitle(f'CLS Attention â€” All {num_layers} Layers (mean over heads)\\n{_title(true_label, pred_label)}',\n                 fontsize=12, fontweight='bold')\n\n    axes[0].imshow(img); axes[0].set_title('Input', fontsize=9); axes[0].axis('off')\n\n    for i, aw in enumerate(all_attn_weights):\n        cls_attn = aw[0].numpy().mean(axis=0)[0, 1:]\n        attn_map = _normalise_map(cls_attn.reshape(grid, grid))\n        ax = axes[i + 1]\n        ax.imshow(img)\n        ax.imshow(attn_map, cmap='hot', alpha=0.6,\n                  extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\n        ax.set_title(f'Layer {i + 1}', fontsize=9); ax.axis('off')\n\n    for i in range(num_layers + 1, len(axes)):\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f'Saved: {save_path}')\n    plt.show()\n    plt.close(fig)\n\n\nplot_all_layers_cls_attention(\n    img_tensor, all_attn_weights, true_label, pred_label,\n    save_path=f'{SAVE_DIR}/attn_all_layers.png'\n)",
   "id": "207033bf9dc3ffc6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ“Š Plot 3 â€” Attention Rollout",
   "id": "97d028a3b3a4a118"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_attention_rollout(img_tensor, all_attn_weights, true_label, pred_label, save_path=None):\n    grid = _grid(all_attn_weights[0])\n    img  = denorm(img_tensor[0])\n\n    rollout  = attention_rollout(all_attn_weights)\n    attn_map = _normalise_map(rollout.reshape(grid, grid))\n\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    fig.suptitle(f'Attention Rollout\\n{_title(true_label, pred_label)}',\n                 fontsize=12, fontweight='bold')\n\n    axes[0].imshow(img); axes[0].set_title('Input Image', fontsize=10); axes[0].axis('off')\n\n    hm = axes[1].imshow(attn_map, cmap='hot', interpolation='bilinear')\n    axes[1].set_title(f'Rollout Map ({grid}Ã—{grid} patches)', fontsize=10); axes[1].axis('off')\n    plt.colorbar(hm, ax=axes[1], fraction=0.046, pad=0.04)\n\n    axes[2].imshow(img)\n    axes[2].imshow(attn_map, cmap='hot', alpha=0.6,\n                   extent=[0, img.shape[1], img.shape[0], 0], interpolation='bilinear')\n    axes[2].set_title('Overlay', fontsize=10); axes[2].axis('off')\n\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        print(f'Saved: {save_path}')\n    plt.show()\n    plt.close(fig)\n\n\nplot_attention_rollout(\n    img_tensor, all_attn_weights, true_label, pred_label,\n    save_path=f'{SAVE_DIR}/attn_rollout.png'\n)",
   "id": "42e2e599adc38209"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ’¾ Step 8 â€” Download Plots",
   "id": "4d604e14316215c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = '/kaggle/working/attention_maps.zip'\n",
    "with zipfile.ZipFile(zip_path, 'w') as zf:\n",
    "    for f in Path(SAVE_DIR).glob('*.png'):\n",
    "        zf.write(f, f.name)\n",
    "\n",
    "print(f'âœ“ Zipped all plots â†’ {zip_path}')\n",
    "print('Download via: Kaggle notebook â†’ Output tab â†’ attention_maps.zip')"
   ],
   "id": "727887ae37f2626c"
  }
 ]
}
